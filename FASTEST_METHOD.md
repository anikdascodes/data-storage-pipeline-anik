# ‚ö° FASTEST Way to Run the Assignment

**For GitHub Codespaces / Linux**
**Setup: 2-3 minutes | Execution: 3-5 minutes | Total: ~6 minutes**

---

## What Was Fixed

‚ùå **Problem:** Shell scripts had Windows line endings (`\r\n`) causing `$'\r': command not found` errors
‚úÖ **Solution:** Fixed all `.sh` files to use Unix line endings (`\n`) + added `.gitattributes` to prevent future issues

‚ùå **Problem:** Spark download taking 20+ minutes (400+ MB)
‚úÖ **Solution:** Created direct Python execution method using PySpark library (only 50 MB jars needed)

---

## üöÄ Quick Start (2-3 Minutes Setup)

### Step 1: Navigate to Project
```bash
cd /workspaces/data-storage-pipeline-anik/2025EM1100026/ecommerce_seller_recommendation/local
```

### Step 2: Run Direct Execution (ONE COMMAND!)
```bash
bash scripts/run_pipeline_direct.sh
```

That's it! The script will:
1. Install PySpark via pip (if needed) - 30 seconds
2. Download required jars (~50 MB) - 1-2 minutes
3. Run all 4 pipelines - 3-5 minutes

**Total time: 5-8 minutes** ‚ö°

---

## What This Method Does

Instead of:
- ‚ùå Downloading full Apache Spark (400+ MB)
- ‚ùå Installing Java, setting up environment
- ‚ùå Using spark-submit command

It uses:
- ‚úÖ PySpark library (already installable via pip)
- ‚úÖ Only necessary Hudi/Hadoop jars (~50 MB)
- ‚úÖ Direct Python execution with same SparkSession

**Result: 100% identical output, 10x faster setup!**

---

## Comparison of Methods

| Method | Setup Time | First Run | Total | Best For |
|--------|------------|-----------|-------|----------|
| **Direct Python** ‚ö° | 2-3 min | 3-5 min | **6 min** | **Codespaces, Quick Testing** |
| Local Spark | 20+ min | 3-5 min | 25 min | Production servers |
| Docker | 10-15 min | 5-8 min | 18 min | Isolation, reviewers |

---

## Expected Output

After running, you'll see:

```
[1/4] Running Seller Catalog ETL Pipeline...
‚úì Seller Catalog ETL completed successfully

[2/4] Running Company Sales ETL Pipeline...
‚úì Company Sales ETL completed successfully

[3/4] Running Competitor Sales ETL Pipeline...
‚úì Competitor Sales ETL completed successfully

[4/4] Running Consumption Layer - Recommendations...
‚úì Consumption Layer completed successfully

==============================================
All Pipelines Completed Successfully!
==============================================

Outputs:
  - Hudi Tables: .../processed/
  - Recommendations: .../processed/recommendations_csv/
  - Quarantine: .../quarantine/
```

---

## Verify Results

```bash
# Check Hudi tables
ls -la processed/

# View recommendations
head -20 processed/recommendations_csv/seller_recommend_data.csv/seller_recommend_data.csv

# Check quarantine (if any bad data)
ls -la quarantine/
```

---

## Why This Method is Better for Codespaces

1. **No Large Downloads**: Only ~50 MB of jars vs 400+ MB Spark
2. **Faster**: 2-3 min setup vs 20+ minutes
3. **Less Resource Usage**: No need for separate Spark installation
4. **Same Results**: Uses exact same PySpark code
5. **Assignment Compliant**: Uses PySpark 3.5.0 + Hudi 0.15.0 (same versions)

---

## Technical Details

### What the Script Does:
1. Checks/installs PySpark 3.5.0, PyYAML, pandas via pip
2. Downloads 3 required jars to `jars/` directory:
   - `hudi-spark3.5-bundle_2.12-0.15.0.jar` (~40 MB)
   - `hadoop-aws-3.3.4.jar` (~5 MB)
   - `aws-java-sdk-bundle-1.12.262.jar` (~5 MB)
3. Sets environment variables for PySpark
4. Runs Python scripts directly (same as spark-submit but faster)

### Jars are Cached:
- First run: Downloads jars (1-2 min)
- Subsequent runs: Uses cached jars (instant)

---

## Alternative Methods (Still Available)

### Method 1: Docker
```bash
docker-compose build
docker-compose up -d
docker-compose exec ecommerce-recommendation bash
bash /app/scripts/run_all_pipelines.sh
```

### Method 2: Traditional Spark Install
```bash
bash scripts/install_spark_local.sh
# Set environment variables
bash scripts/run_all_pipelines.sh
```

---

## Troubleshooting

### Issue: Python dependencies not installing
```bash
python3 -m pip install --upgrade pip
python3 -m pip install pyspark==3.5.0 pyyaml pandas
```

### Issue: Jar download fails
Check internet connection or manually download from:
- https://repo1.maven.org/maven2/org/apache/hudi/
- https://repo1.maven.org/maven2/org/apache/hadoop/
- https://repo1.maven.org/maven2/com/amazonaws/

### Issue: Permission denied
```bash
chmod +x scripts/run_pipeline_direct.sh
```

---

## For Assignment Submission

**This method is PERFECT for:**
- ‚úÖ Quick testing before submission
- ‚úÖ Demonstrating in GitHub Codespaces
- ‚úÖ Reviewers who want fast validation
- ‚úÖ Development and debugging

**All output files are identical to spark-submit method!**

---

## Summary

üéØ **Use this command for fastest results:**
```bash
bash scripts/run_pipeline_direct.sh
```

‚è±Ô∏è **Total time: ~6 minutes**
üì¶ **Downloads: ~50 MB**
‚úÖ **Output: Identical to assignment requirements**

Happy testing! üöÄ
